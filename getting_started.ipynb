{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "---\n",
    "\n",
    "## Web scraping and analysis\n",
    "\n",
    "This Jupyter notebook includes some code to get you started with web scraping. We will use a package called `BeautifulSoup` to collect the data from the web. Once you've collected your data and saved it into a local `.csv` file you should start with your analysis.\n",
    "\n",
    "### Scraping data from Skytrax\n",
    "\n",
    "If you visit [https://www.airlinequality.com] you can see that there is a lot of data there. For this task, we are only interested in reviews related to British Airways and the Airline itself.\n",
    "\n",
    "If you navigate to this link: [https://www.airlinequality.com/airline-reviews/british-airways] you will see this data. Now, we can use `Python` and `BeautifulSoup` to collect all the links to the reviews and then to collect the text data on each of the individual review links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1\n",
      "   ---> 100 total reviews\n",
      "   ---> 100 total recommendations\n",
      "Scraping page 2\n",
      "   ---> 200 total reviews\n",
      "   ---> 200 total recommendations\n",
      "Scraping page 3\n",
      "   ---> 300 total reviews\n",
      "   ---> 300 total recommendations\n",
      "Scraping page 4\n",
      "   ---> 400 total reviews\n",
      "   ---> 400 total recommendations\n",
      "Scraping page 5\n",
      "   ---> 500 total reviews\n",
      "   ---> 500 total recommendations\n",
      "Scraping page 6\n",
      "   ---> 600 total reviews\n",
      "   ---> 600 total recommendations\n",
      "Scraping page 7\n",
      "   ---> 700 total reviews\n",
      "   ---> 700 total recommendations\n",
      "Scraping page 8\n",
      "   ---> 800 total reviews\n",
      "   ---> 800 total recommendations\n",
      "Scraping page 9\n",
      "   ---> 900 total reviews\n",
      "   ---> 900 total recommendations\n",
      "Scraping page 10\n",
      "   ---> 1000 total reviews\n",
      "   ---> 1000 total recommendations\n",
      "Scraping page 11\n",
      "   ---> 1100 total reviews\n",
      "   ---> 1100 total recommendations\n",
      "Scraping page 12\n",
      "   ---> 1200 total reviews\n",
      "   ---> 1200 total recommendations\n",
      "Scraping page 13\n",
      "   ---> 1300 total reviews\n",
      "   ---> 1300 total recommendations\n",
      "Scraping page 14\n",
      "   ---> 1400 total reviews\n",
      "   ---> 1400 total recommendations\n",
      "Scraping page 15\n",
      "   ---> 1500 total reviews\n",
      "   ---> 1500 total recommendations\n",
      "Scraping page 16\n",
      "   ---> 1600 total reviews\n",
      "   ---> 1600 total recommendations\n",
      "Scraping page 17\n",
      "   ---> 1700 total reviews\n",
      "   ---> 1700 total recommendations\n",
      "Scraping page 18\n",
      "   ---> 1800 total reviews\n",
      "   ---> 1800 total recommendations\n",
      "Scraping page 19\n",
      "   ---> 1900 total reviews\n",
      "   ---> 1900 total recommendations\n",
      "Scraping page 20\n",
      "   ---> 2000 total reviews\n",
      "   ---> 2000 total recommendations\n",
      "Scraping page 21\n",
      "   ---> 2100 total reviews\n",
      "   ---> 2100 total recommendations\n",
      "Scraping page 22\n",
      "   ---> 2200 total reviews\n",
      "   ---> 2200 total recommendations\n",
      "Scraping page 23\n",
      "   ---> 2300 total reviews\n",
      "   ---> 2300 total recommendations\n",
      "Scraping page 24\n",
      "   ---> 2400 total reviews\n",
      "   ---> 2400 total recommendations\n",
      "Scraping page 25\n",
      "   ---> 2500 total reviews\n",
      "   ---> 2500 total recommendations\n",
      "Scraping page 26\n",
      "   ---> 2600 total reviews\n",
      "   ---> 2600 total recommendations\n",
      "Scraping page 27\n",
      "   ---> 2700 total reviews\n",
      "   ---> 2700 total recommendations\n",
      "Scraping page 28\n",
      "   ---> 2800 total reviews\n",
      "   ---> 2800 total recommendations\n",
      "Scraping page 29\n",
      "   ---> 2900 total reviews\n",
      "   ---> 2900 total recommendations\n",
      "Scraping page 30\n",
      "   ---> 3000 total reviews\n",
      "   ---> 3000 total recommendations\n",
      "Total reviews scraped: 3000\n",
      "Total recommendations scraped: 3000\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://www.airlinequality.com/airline-reviews/british-airways\"\n",
    "pages = 30\n",
    "page_size = 100\n",
    "reviews = []\n",
    "recommendations = []\n",
    "for i in range(1, pages + 1):\n",
    "    print(f\"Scraping page {i}\")\n",
    "    # Create URL to collect links from paginated data\n",
    "    url = f\"{base_url}/page/{i}/?sortby=post_date%3ADesc&pagesize={page_size}\"\n",
    "    # Collect HTML data from this page\n",
    "    response = requests.get(url)\n",
    "    # Parse content\n",
    "    content = response.content\n",
    "    parsed_content = BeautifulSoup(content, 'html.parser')\n",
    "    # Find all review div elements and append their text content to the 'reviews' list\n",
    "    for para in parsed_content.find_all(\"div\", {\"class\": \"text_content\"}):\n",
    "        reviews.append(para.get_text())\n",
    "    # Find all recommendation elements and append their text content to the 'recommendations' list\n",
    "    recommendation_tags = parsed_content.find_all(string=[\"no\", \"yes\"])\n",
    "    for rec in recommendation_tags:\n",
    "        recommendations.append(rec)\n",
    "    print(f\"   ---> {len(reviews)} total reviews\")\n",
    "    print(f\"   ---> {len(recommendations)} total recommendations\")\n",
    "# Print the number of reviews and recommendations scraped in total\n",
    "print(f\"Total reviews scraped: {len(reviews)}\")\n",
    "print(f\"Total recommendations scraped: {len(recommendations)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>recommendations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>✅ Trip Verified | London Heathrow to Mumbai in...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>✅ Trip Verified | Keflavík, Iceland to London ...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>✅ Trip Verified | Terrible Experience with Bri...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>✅ Trip Verified | An airline that lives in the...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>✅ Trip Verified |  Check-in Desk rude and dism...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews recommendations\n",
       "0  ✅ Trip Verified | London Heathrow to Mumbai in...             yes\n",
       "1  ✅ Trip Verified | Keflavík, Iceland to London ...             yes\n",
       "2  ✅ Trip Verified | Terrible Experience with Bri...              no\n",
       "3  ✅ Trip Verified | An airline that lives in the...              no\n",
       "4  ✅ Trip Verified |  Check-in Desk rude and dism...              no"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"reviews\"] = reviews\n",
    "df[\"recommendations\"] = recommendations\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(\"data/BA_reviews.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! Now you have your dataset for this task! The loops above collected 3000 reviews by iterating through the paginated pages on the website. However, if you want to collect more data, try increasing the number of pages!\n",
    "\n",
    " The next thing that you should do is clean this data to remove any unnecessary text from each of the rows. For example, \"✅ Trip Verified\" can be removed from each row if it exists, as it's not relevant to what we want to investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f7924c4c56b083e0e50eadfe7ef592a7a8ef70df33a0047f82280e6be1afe15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
